{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n90H9I0cMQvY"
      },
      "source": [
        "------\n",
        "### Library setup and mounting g-drive \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DqCV-Yrk0o5",
        "outputId": "03a4c23b-b278-40c0-a5ef-441535aa8fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlICaYzQan59"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/huggingface/nlp\n",
        "!pip install --quiet transformers\n",
        "!pip install --quiet nlp==0.2.0\n",
        "!pip install --quiet datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejhYiFELazOd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import nlp\n",
        "from sklearn import preprocessing\n",
        "# from torch.utils.data import Dataset\n",
        "from datasets import load_dataset, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "logging.basicConfig(level=logging.CRITICAL)\n",
        "\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXdgg0ucNGvj"
      },
      "source": [
        "------\n",
        "### Fetching our data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvxSRaQFrWC1"
      },
      "outputs": [],
      "source": [
        "# paths to semeval dataset\n",
        "path_to_longform_textrank_data = \"../dataset/semeval/\"\n",
        "path_to_train = f\"{path_to_longform_textrank_data}train.csv\"\n",
        "path_to_test = f\"{path_to_longform_textrank_data}test.csv\"\n",
        "path_to_validation = f\"{path_to_longform_textrank_data}final_test_v1.csv\"\n",
        "\n",
        "# paths to fnc dataset\n",
        "path_to_fnc_data = \"../dataset/FNC/\"\n",
        "path_to_fnc_train = f\"{path_to_fnc_data}train_v2.csv\"\n",
        "path_to_fnc_validation = f\"{path_to_fnc_data}validation.csv\"\n",
        "path_to_fnc_test = f\"{path_to_fnc_data}test_v2.csv\"\n",
        "\n",
        "# path to hyperpartisan pan data\n",
        "path_to_pan_data = \"../dataset/hyperpartisan_dataset/\"\n",
        "path_to_pan_train = f\"{path_to_pan_data}train.csv\"\n",
        "path_to_pan_validation = f\"{path_to_pan_data}validation.csv\"\n",
        "path_to_pan_test = f\"{path_to_pan_data}test.csv\"\n",
        "\n",
        "# path to fnid data\n",
        "path_to_fnid = \"../dataset/FNID-dataset/\"\n",
        "path_to_fnid_train = f\"{path_to_fnid}liar_train_processed.csv\"\n",
        "path_to_fnid_validation = f\"{path_to_fnid}liar_dev_processed.csv\"\n",
        "path_to_fnid_test = f\"{path_to_fnid}liar_test_processed.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pntb5EqgapA1"
      },
      "outputs": [],
      "source": [
        "dataset_dict = {\n",
        "  \"stsb\": load_dataset('glue', name='stsb'),\n",
        "  \"semeval\": load_dataset('csv', data_files={\n",
        "      'train': path_to_train,\n",
        "      'validation': path_to_validation,\n",
        "      'test': path_to_test,\n",
        "  }),\n",
        "  \"fnc\": load_dataset('csv', data_files={\n",
        "      'train': path_to_fnc_train,\n",
        "      'validation': path_to_fnc_validation,\n",
        "      'test': path_to_fnc_test,\n",
        "  }),\n",
        "  \"fnid\": load_dataset('csv', data_files={\n",
        "      'train': path_to_fnid_train,\n",
        "      'validation': path_to_fnc_validation,\n",
        "      'test': path_to_fnid_test,\n",
        "  }),\n",
        "  \"pan\": load_dataset('csv', data_files={\n",
        "      'train': path_to_pan_train,\n",
        "      'validation': path_to_pan_validation,\n",
        "      'test': path_to_pan_test,\n",
        "  })\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlFMhcMUPpBE"
      },
      "outputs": [],
      "source": [
        "dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFTXc2iVOizO"
      },
      "source": [
        "We can show one example from each task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiP4yQMPOQVz"
      },
      "outputs": [],
      "source": [
        "for task_name, dataset in dataset_dict.items():\n",
        "    print(task_name)\n",
        "    print(dataset_dict[task_name][\"train\"][0])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smro5_4pA7Hm"
      },
      "source": [
        "------\n",
        "### Creating multitask training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVX5hFlzmLka"
      },
      "outputs": [],
      "source": [
        "class MultitaskModel(transformers.PreTrainedModel):\n",
        "    def __init__(self, encoder, taskmodels_dict):\n",
        "        \"\"\"\n",
        "        Setting MultitaskModel up as a PretrainedModel allows us\n",
        "        to take better advantage of Trainer features\n",
        "        \"\"\"\n",
        "        super().__init__(transformers.PretrainedConfig())\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, model_name, model_type_dict, model_config_dict):\n",
        "        \"\"\"\n",
        "        This creates a MultitaskModel using the model class and config objects\n",
        "        from single-task models. \n",
        "\n",
        "        We do this by creating each single-task model, and having them share\n",
        "        the same encoder transformer.\n",
        "        \"\"\"\n",
        "        shared_encoder = None\n",
        "        taskmodels_dict = {}\n",
        "        for task_name, model_type in model_type_dict.items():\n",
        "            model = model_type.from_pretrained(\n",
        "                model_name, \n",
        "                config=model_config_dict[task_name],\n",
        "            )\n",
        "            if shared_encoder is None:\n",
        "                print(cls.get_encoder_attr_name(model))\n",
        "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
        "            else:\n",
        "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
        "            taskmodels_dict[task_name] = model\n",
        "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def get_encoder_attr_name(cls, model):\n",
        "        \"\"\"\n",
        "        The encoder transformer is named differently in each model \"architecture\".\n",
        "        This method lets us get the name of the encoder attribute\n",
        "        \"\"\"\n",
        "        model_class_name = model.__class__.__name__\n",
        "        if model_class_name.startswith(\"Bert\"):\n",
        "            return \"bert\"\n",
        "        elif model_class_name.startswith(\"Roberta\"):\n",
        "            return \"roberta\"\n",
        "        elif model_class_name.startswith(\"Albert\"):\n",
        "            return \"albert\"\n",
        "        elif model_class_name.startswith(\"Deberta\"):\n",
        "            return \"deberta\"\n",
        "        else:\n",
        "            raise KeyError(f\"Add support for new model {model_class_name}\")\n",
        "\n",
        "    def forward(self, task_name, **kwargs):\n",
        "        return self.taskmodels_dict[task_name](**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zSZsp8Cb7gd"
      },
      "source": [
        "The `MultitaskModel` class consists of only two components - the shared \"encoder\", a dictionary to the individual task models. Now, we can simply create the corresponding task models by supplying the invidual model classes and model configs. We will use Transformers' AutoModels to further automate the choice of model class given a model architecture (in our case, let's use `microsoft/deberta-base`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_sosFINo24B"
      },
      "outputs": [],
      "source": [
        "model_name = \"microsoft/deberta-base\"\n",
        "multitask_model = MultitaskModel.create(\n",
        "    model_name=model_name,\n",
        "    model_type_dict={\n",
        "        \"stsb\": transformers.AutoModelForSequenceClassification,\n",
        "        \"semeval\": transformers.AutoModelForSequenceClassification,\n",
        "        \"pan\": transformers.AutoModelForSequenceClassification,\n",
        "        \"fnid\": transformers.AutoModelForSequenceClassification,\n",
        "    },\n",
        "    model_config_dict={\n",
        "        \"stsb\": transformers.AutoConfig.from_pretrained(model_name, num_labels=1),\n",
        "        \"semeval\": transformers.AutoConfig.from_pretrained(model_name, num_labels=1),\n",
        "        \"pan\": transformers.AutoConfig.from_pretrained(model_name, num_labels=6),\n",
        "        \"fnid\": transformers.AutoConfig.from_pretrained(model_name, num_labels=1),\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxSAQ6q4O-uL"
      },
      "source": [
        "To confirm that all three task-models use the same encoder, we can check the data pointers of the respective encoders. In this case, we'll check that the word embeddings in each model all point to the same memory location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrtS3ZeSsoZw"
      },
      "outputs": [],
      "source": [
        "if model_name.startswith(\"microsoft/deberta-base\"):\n",
        "    print(multitask_model.encoder.embeddings.word_embeddings.weight.data_ptr())\n",
        "    print(multitask_model.taskmodels_dict[\"stsb\"].deberta.embeddings.word_embeddings.weight.data_ptr())\n",
        "    print(multitask_model.taskmodels_dict[\"semeval\"].deberta.embeddings.word_embeddings.weight.data_ptr())\n",
        "    # print(multitask_model.taskmodels_dict[\"fnc\"].deberta.embeddings.word_embeddings.weight.data_ptr())\n",
        "    print(multitask_model.taskmodels_dict[\"pan\"].deberta.embeddings.word_embeddings.weight.data_ptr())\n",
        "    print(multitask_model.taskmodels_dict[\"fnid\"].deberta.embeddings.word_embeddings.weight.data_ptr())\n",
        "else:\n",
        "    print(\"Exercise for the reader: add a check for other model architectures =)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n96ifPukDkb"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYnt_DEEAzoJ"
      },
      "source": [
        "------\n",
        "### Preparation of dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSeBYKCublmo"
      },
      "outputs": [],
      "source": [
        "max_length = 128\n",
        "\n",
        "def convert_to_stsb_features(example_batch):\n",
        "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
        "    features = tokenizer.batch_encode_plus(\n",
        "        inputs, max_length=max_length, padding=\"max_length\"\n",
        "    )\n",
        "    features[\"labels\"] = list(np.float_(example_batch[\"label\"]))\n",
        "    return features\n",
        "\n",
        "def convert_to_semeval_features(example_batch):\n",
        "    inputs = list(zip(example_batch['sentence1'], example_batch['sentence2']))\n",
        "    features = tokenizer.batch_encode_plus(\n",
        "        inputs, max_length=512, padding=\"max_length\"\n",
        "    )\n",
        "    features[\"labels\"] = example_batch[\"label\"]\n",
        "    return features\n",
        "\n",
        "def convert_to_fnc_features(example_batch):\n",
        "    inputs = list(zip(example_batch['Headline'], example_batch['articleBody']))\n",
        "    features = tokenizer.batch_encode_plus(\n",
        "        inputs, max_length=max_length, padding=\"max_length\"\n",
        "    )\n",
        "    features[\"labels\"] = example_batch[\"Stance\"]\n",
        "    return features\n",
        "\n",
        "def convert_to_fnid_features(example_batch):\n",
        "    inputs = list(zip(example_batch['statement'], example_batch['fullText_based_content']))\n",
        "    features = tokenizer.batch_encode_plus(\n",
        "        inputs, max_length=max_length, padding=\"max_length\"\n",
        "    )\n",
        "    features[\"labels\"] = example_batch[\"label-liar\"]\n",
        "    return features\n",
        "\n",
        "def convert_to_pan_features(example_batch):\n",
        "    features = tokenizer.batch_encode_plus(\n",
        "        example_batch[\"text\"], max_length=max_length, padding=\"max_length\"\n",
        "    )\n",
        "    features[\"labels\"] = example_batch[\"bias\"]\n",
        "    return features\n",
        "\n",
        "convert_func_dict = {\n",
        "    \"stsb\": convert_to_stsb_features,\n",
        "    \"fnid\": convert_to_fnid_features,\n",
        "    \"pan\": convert_to_pan_features,\n",
        "    \"semeval\": convert_to_semeval_features,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eULIAQ9WRYXn"
      },
      "source": [
        "Now that we have defined the above functions, we can use `dataset.map` method available in the NLP library to apply the functions over our entire datasets. The NLP library that handles the mapping efficiently and caches the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcLnS85Hkhbf"
      },
      "outputs": [],
      "source": [
        "columns_dict = {\n",
        "    \"stsb\": ['input_ids', 'attention_mask', 'labels'],\n",
        "    \"semeval\": ['input_ids', 'attention_mask', 'labels'],\n",
        "    \"pan\": ['input_ids', 'attention_mask', 'labels'],\n",
        "    \"fnid\": ['input_ids', 'attention_mask', 'labels'],\n",
        "}\n",
        "\n",
        "features_dict = {}\n",
        "for task_name, dataset in dataset_dict.items():\n",
        "    print(u\"\\u2192\", task_name)\n",
        "    features_dict[task_name] = {}\n",
        "    for phase, phase_dataset in dataset.items():\n",
        "        features_dict[task_name][phase] = phase_dataset.map(\n",
        "            convert_func_dict[task_name],\n",
        "            batched=True,\n",
        "            load_from_cache_file=False,\n",
        "        )\n",
        "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))\n",
        "        features_dict[task_name][phase].set_format(\n",
        "            type=\"torch\", \n",
        "            columns=columns_dict[task_name],\n",
        "        )\n",
        "        print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4N7IF9hTR7m"
      },
      "source": [
        "## Preparing a multi-task data loader and Trainer\n",
        "\n",
        "Setting up a multi-task data loader should be simple in principle - we simply need to sample from multiple single-task data loaders with some probability, and feed each batch to the multi-task model above. Of course, along with each batch, we also need to tell the model what task it is for, so `MultitaskModel` knows to use the right corresponding task-model.\n",
        "\n",
        "However, because we want to use the built-in `Trainer` class in Transformers, this gets a little tricky, since the `Trainer` expects a single data loader, and expects a very specific format of per-batch data. This slice of code is somewhat of a hack around that constraint. (This can become a lot more streamlined with some tweaks to the Trainer code from the Hugging Face folks =))\n",
        "\n",
        "We need to define a `MultitaskDataloader` that combines several data loaders into a single \"data loader\" - not so different from our multi-task model above! This `MultitaskDataloader` should do what we described: sample from different single-task data loaders, and yield a task batch and the corresponding task name (we're going to add the `task_name` to the batch data).\n",
        "\n",
        "We will also need to override the `get_train_dataloader` method of the `Trainer` to play well with our `MultitaskDataloader`. We do this with a `MultitaskTrainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdUbpAIZzWce"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from transformers.data.data_collator import DataCollator, InputDataClass, DefaultDataCollator\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "from typing import List, Union, Dict\n",
        "\n",
        "\n",
        "class NLPDataCollator(DefaultDataCollator):\n",
        "    \"\"\"\n",
        "    Extending the existing DataCollator to work with NLP dataset batches\n",
        "    \"\"\"\n",
        "    def collate_batch(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
        "        first = features[0]\n",
        "        if isinstance(first, dict):\n",
        "          # NLP data sets current works presents features as lists of dictionary\n",
        "          # (one per example), so we  will adapt the collate_batch logic for that\n",
        "          if \"labels\" in first and first[\"labels\"] is not None:\n",
        "              if first[\"labels\"].dtype == torch.int64:\n",
        "                  labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
        "              else:\n",
        "                  labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
        "              batch = {\"labels\": labels}\n",
        "          for k, v in first.items():\n",
        "              if k != \"labels\" and v is not None and not isinstance(v, str):\n",
        "                  batch[k] = torch.stack([f[k] for f in features])\n",
        "          return batch\n",
        "        else:\n",
        "          # otherwise, revert to using the default collate_batch\n",
        "          return DefaultDataCollator().collate_batch(features)\n",
        "\n",
        "\n",
        "class StrIgnoreDevice(str):\n",
        "    \"\"\"\n",
        "    This is a hack. The Trainer is going call .to(device) on every input\n",
        "    value, but we need to pass in an additional `task_name` string.\n",
        "    This prevents it from throwing an error\n",
        "    \"\"\"\n",
        "    def to(self, device):\n",
        "        return self\n",
        "\n",
        "\n",
        "class DataLoaderWithTaskname:\n",
        "    \"\"\"\n",
        "    Wrapper around a DataLoader to also yield a task name\n",
        "    \"\"\"\n",
        "    def __init__(self, task_name, data_loader):\n",
        "        self.task_name = task_name\n",
        "        self.data_loader = data_loader\n",
        "\n",
        "        self.batch_size = data_loader.batch_size\n",
        "        self.dataset = data_loader.dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_loader)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for batch in self.data_loader:\n",
        "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
        "            yield batch\n",
        "\n",
        "\n",
        "class MultitaskDataloader:\n",
        "    \"\"\"\n",
        "    Data loader that combines and samples from multiple single-task\n",
        "    data loaders.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataloader_dict):\n",
        "        self.dataloader_dict = dataloader_dict\n",
        "        self.num_batches_dict = {\n",
        "            task_name: len(dataloader) \n",
        "            for task_name, dataloader in self.dataloader_dict.items()\n",
        "        }\n",
        "        self.task_name_list = list(self.dataloader_dict)\n",
        "        self.dataset = [None] * sum(\n",
        "            len(dataloader.dataset) \n",
        "            for dataloader in self.dataloader_dict.values()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(self.num_batches_dict.values())\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        For each batch, sample a task, and yield a batch from the respective\n",
        "        task Dataloader.\n",
        "\n",
        "        We use size-proportional sampling, but you could easily modify this\n",
        "        to sample from some-other distribution.\n",
        "        \"\"\"\n",
        "        task_choice_list = []\n",
        "        for i, task_name in enumerate(self.task_name_list):\n",
        "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
        "        task_choice_list = np.array(task_choice_list)\n",
        "        np.random.shuffle(task_choice_list)\n",
        "        dataloader_iter_dict = {\n",
        "            task_name: iter(dataloader) \n",
        "            for task_name, dataloader in self.dataloader_dict.items()\n",
        "        }\n",
        "        for task_choice in task_choice_list:\n",
        "            task_name = self.task_name_list[task_choice]\n",
        "            yield next(dataloader_iter_dict[task_name])    \n",
        "\n",
        "class MultitaskTrainer(transformers.Trainer):\n",
        "\n",
        "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
        "        \"\"\"\n",
        "        Create a single-task data loader that also yields task names\n",
        "        \"\"\"\n",
        "        if self.train_dataset is None:\n",
        "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
        "        else:\n",
        "            train_sampler = (\n",
        "                RandomSampler(train_dataset)\n",
        "                if self.args.local_rank == -1\n",
        "                else DistributedSampler(train_dataset)\n",
        "            )\n",
        "\n",
        "        data_loader = DataLoaderWithTaskname(\n",
        "            task_name=task_name,\n",
        "            data_loader=DataLoader(\n",
        "              train_dataset,\n",
        "              batch_size=self.args.train_batch_size,\n",
        "              sampler=train_sampler,\n",
        "              collate_fn=self.data_collator.collate_batch,\n",
        "            ),\n",
        "        )\n",
        "        return data_loader\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "        \"\"\"\n",
        "        Returns a MultitaskDataloader, which is not actually a Dataloader\n",
        "        but an iterable that returns a generator that samples from each \n",
        "        task Dataloader\n",
        "        \"\"\"\n",
        "        return MultitaskDataloader({\n",
        "            task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
        "            for task_name, task_dataset in self.train_dataset.items()\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AtQfkWxVUSJ"
      },
      "source": [
        "## Time to train!\n",
        "\n",
        "Okay, we have done all the hard work, now it is time for it to pay off. We can now simply create our `MultitaskTrainer`, and start training! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4YUxdIZz3_i"
      },
      "outputs": [],
      "source": [
        "train_dataset = {\n",
        "    task_name: dataset[\"train\"] \n",
        "    for task_name, dataset in features_dict.items()\n",
        "}\n",
        "trainer = MultitaskTrainer(\n",
        "    model=multitask_model,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=\"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/train_models/longformer/model_pan_fnid_deberta\",\n",
        "        overwrite_output_dir=True,\n",
        "        learning_rate=1e-5,\n",
        "        do_train=True,\n",
        "        num_train_epochs=2,\n",
        "        # Adjust batch size if this doesn't fit on the Colab GPU\n",
        "        per_device_train_batch_size=2,  \n",
        "        save_steps=30000,\n",
        "    ),\n",
        "    data_collator=NLPDataCollator(),\n",
        "    train_dataset=train_dataset,\n",
        ") \n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzG-IKzpBVH1"
      },
      "source": [
        "-----\n",
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2eYxSeNc4mr"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "preds = defaultdict(list)\n",
        "batch_size = 1\n",
        "task_name = 'semeval'\n",
        "dataset = 'test'\n",
        "val_len = len(features_dict[task_name][dataset])\n",
        "\n",
        "for index in tqdm(range(0, val_len, batch_size), total=val_len):\n",
        "\n",
        "    idx = features_dict[task_name][dataset][index]\n",
        "    inputs = features_dict[task_name][dataset][index]\n",
        "\n",
        "    overall, attention_mask, input_ids = inputs['labels'], inputs['attention_mask'].unsqueeze(0).to(\"cuda\"), inputs['input_ids'].unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "    args = {'input_ids': input_ids, 'attention_mask': attention_mask}   \n",
        "    overall_pred = multitask_model(task_name, **args)\n",
        "\n",
        "\n",
        "    preds['idx'].append(features_dict[task_name][dataset]['idx'][index])\n",
        "    preds['overall'].append(overall_pred['logits'].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2gJTimLUGn5"
      },
      "outputs": [],
      "source": [
        "# Evalute Semeval data\n",
        "nlp.load_metric('glue', name=\"stsb\").compute(\n",
        "    preds['overall'],\n",
        "    features_dict[task_name][dataset]['labels'].tolist(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y8MI9BXnGAg"
      },
      "outputs": [],
      "source": [
        "def check_pred(pred):\n",
        "  if pred >= 4:\n",
        "    return 4.0000\n",
        "  elif pred <= 1:\n",
        "    return 1.0000\n",
        "  else:\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTFulBbRot1I"
      },
      "outputs": [],
      "source": [
        "# Prediction on test data\n",
        "# retrieve the test_v1.csv file\n",
        "main_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/test_v1.csv')\n",
        "predictions = preds['overall']\n",
        "df = pd.read_csv(path_to_validation)\n",
        "pair_ids = df['pair_id']\n",
        "# create new dataframe\n",
        "pred_df = pd.DataFrame({\n",
        "    'pair_id': pair_ids,\n",
        "    'Overall': predictions,\n",
        "})\n",
        "# merge data\n",
        "merged_data = pd.merge(pred_df, main_df, how=\"outer\", on=\"pair_id\")\n",
        "merged_data.drop(['url1_lang', 'url2_lang', 'link1', 'link2', 'ia_link1', 'ia_link2'], axis=1, inplace=True)\n",
        "merged_data['Overall'] = merged_data['Overall'].round(4)\n",
        "# save data\n",
        "merged_data.to_csv('prediction.csv', index=False)\n",
        "\n",
        "# processing\n",
        "merged_data[\"Proc_Overall\"] = merged_data.progress_apply(\n",
        "    lambda row: check_pred(row['Overall']),\n",
        "    axis=1\n",
        ")\n",
        "merged_data.drop(['Overall'], axis=1, inplace=True)\n",
        "merged_data.rename(columns={\n",
        "    \"Proc_Overall\": \"Overall\"\n",
        "}, inplace=True, errors=\"raise\")\n",
        "merged_data.to_csv(\"prediction.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Multi-task Training with Transformers+NLP (15)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
